{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/NLP_Portilia/blob/NLP_Spacy_Basics_1/07_NLP_Basics_Assessment_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7omq1TqAHYV"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF-No2xqAHYX"
      },
      "source": [
        "# NLP Basics Assessment - Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaZTkqD2AHYY"
      },
      "source": [
        "For this assessment we'll be using the short story [_An Occurrence at Owl Creek Bridge_](https://en.wikipedia.org/wiki/An_Occurrence_at_Owl_Creek_Bridge) by Ambrose Bierce (1890). <br>The story is in the public domain; the text file was obtained from [Project Gutenberg](https://www.gutenberg.org/ebooks/375.txt.utf-8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ffmmey6fAHYY"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL to perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSgdkRsaAHYZ"
      },
      "source": [
        "**1. Create a Doc object from the file `owlcreek.txt`**<br>\n",
        "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2tgtBJFoAHYZ"
      },
      "outputs": [],
      "source": [
        "# Enter your code here:\n",
        "\n",
        "with open('/content/owlcreek.txt') as f:\n",
        "    doc = nlp(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfHDKoxnAHYZ",
        "outputId": "ce297e50-9676-4730-d240-6b7fd05dbf79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
              "\n",
              "by Ambrose Bierce\n",
              "\n",
              "I\n",
              "\n",
              "A man stood upon a railroad bridge in northern Alabama, looking down\n",
              "into the swift water twenty feet below.  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Run this cell to verify it worked:\n",
        "\n",
        "doc[:36]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slicing a document\n",
        "Comment (# Run this cell to verify it worked:):\n",
        "\n",
        "This is a comment in Python. Everything after the # symbol is ignored by the Python interpreter.\n",
        "\n",
        "In this case, the comment suggests that running the cell will allow you to verify if the previous operations worked as expected.\n",
        "\n",
        "--------------------------\n",
        "##doc[:36]:\n",
        "\n",
        "This is a slice operation on the doc variable.\n",
        "\n",
        "The : inside the square brackets indicates a slice, which extracts a portion of the sequence (like a string, list, or another sequence type).\n",
        "\n",
        "\n",
        "Specifically, :36 means \"slice from the beginning up to (but not including) index 36.\"\n",
        "\n",
        "So this retrieves the first 36 elements (characters or items) of the doc variable.\n",
        "\n",
        "If doc is a string, it would return the first 36 characters.\n",
        "\n",
        "If it is a list, it returns the first 36 elements of that list.\n",
        "\n",
        "In summary, when you run this line, it displays the first 36 elements of doc to verify that the variable has been correctly processed or populated in earlier parts of the code."
      ],
      "metadata": {
        "id": "UtrM18PiEdfT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9OaHsKTAHYa"
      },
      "source": [
        "**2. How many tokens are contained in the file?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgZ4VWM8AHYb",
        "outputId": "f797c774-04fe-4fb1-8c67-a948c9f96fa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4833"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining len\n",
        "The expression len(doc) returns the length of the variable doc.\n",
        "\n",
        "--------------------------\n",
        "Here's what happens:\n",
        "\n",
        "##len() function:\n",
        "This is a built-in Python function that returns the number of items in an object.\n",
        "\n",
        "The object can be a string, list, tuple, dictionary, or other types that support a length.\n",
        "\n",
        "If doc is a string, len(doc) will return the number of characters in that string.\n",
        "\n",
        "If doc is a list (or another iterable), it will return the number of elements in the list.\n",
        "\n",
        "-------------------------\n",
        "#Example:\n",
        "\n",
        "    doc = \"Hello, world!\"\n",
        "\n",
        "print(len(doc))  # Output: 13 (number of characters including punctuation and spaces)\n",
        "\n",
        "For a list:\n",
        "\n",
        "    doc = [1, 2, 3, 4, 5]\n",
        "print(len(doc))  # Output: 5 (number of elements in the list)\n",
        "\n",
        "In your case, running len(doc) will simply tell you how many elements (or characters) are in doc."
      ],
      "metadata": {
        "id": "DpKksUDkF3DI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0dG_yRfAHYb"
      },
      "source": [
        "**3. How many sentences are contained in the file?**<br>HINT: You'll want to build a list first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWTsrn95AHYb",
        "outputId": "7d05ddeb-1bce-4c59-c7cb-5a36d7788cf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sents = [sent for sent in doc.sents]\n",
        "len(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a List of sentences\n",
        "This code snippet is performing two main operations:\n",
        "\n",
        "--------------------------\n",
        "##List Comprehension (sents = [sent for sent in doc.sents]):\n",
        "\n",
        "It iterates over each item in doc.sents and creates a list sents, where each item is one of the sentences from doc.\n",
        "\n",
        "doc.sents suggests that doc is likely an object from a natural language processing (NLP) library like spaCy, where sents is a generator for sentences in a document.\n",
        "\n",
        "In this case, each sent is one sentence in the document doc.\n",
        "\n",
        "##len(sents):\n",
        "\n",
        "After creating the sents list, calling len(sents) will return the number of sentences in the document doc.\n",
        "\n",
        "--------------------------\n",
        "#Example using spaCy:\n",
        "\n",
        "    import spacy\n",
        "\n",
        "# Load the English model in spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a doc object by processing some text\n",
        "    doc = nlp(\"This is the first sentence. This is the second sentence.\")\n",
        "\n",
        "# Extract sentences from the doc\n",
        "    sents = [sent for sent in doc.sents]\n",
        "\n",
        "# Print the number of sentences\n",
        "    print(len(sents))  # Output: 2\n",
        "    \n",
        "In this example, len(sents) would return 2 because there are two sentences in the document.\n",
        "\n",
        "So, in your case, len(sents) will return the total number of sentences in the doc object."
      ],
      "metadata": {
        "id": "wtzV4TyGG8ot"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7wWnzYAHYc"
      },
      "source": [
        "**4. Print the second sentence in the document**<br> HINT: Indexing starts at zero, and the title counts as the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmrUPo4VAHYc",
        "outputId": "bef17b34-a03c-42aa-b525-a578a9487faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The man's hands were behind\n",
            "his back, the wrists bound with a cord.  \n"
          ]
        }
      ],
      "source": [
        "print(sents[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SjxTCE3AHYc"
      },
      "source": [
        "** 5. For each token in the sentence above, print its `text`, `POS` tag, `dep` tag and `lemma`<br>\n",
        "CHALLENGE: Have values line up in columns in the print output.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teJ63aGDAHYc",
        "outputId": "2a196969-af22-407f-ebe2-d69485a63bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DET det the\n",
            "man NOUN poss man\n",
            "'s PART case 's\n",
            "hands NOUN nsubj hand\n",
            "were AUX ROOT be\n",
            "behind ADP prep behind\n",
            "\n",
            " SPACE dep \n",
            "\n",
            "his PRON poss his\n",
            "back NOUN pobj back\n",
            ", PUNCT punct ,\n",
            "the DET det the\n",
            "wrists NOUN appos wrist\n",
            "bound VERB acl bind\n",
            "with ADP prep with\n",
            "a DET det a\n",
            "cord NOUN pobj cord\n",
            ". PUNCT punct .\n",
            "  SPACE dep  \n"
          ]
        }
      ],
      "source": [
        "# NORMAL SOLUTION:\n",
        "for token in sents[1]:\n",
        "    print(token.text, token.pos_, token.dep_, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print different types of tokens\n",
        "This code snippet is iterating over the tokens in the second sentence (sents[1]) and printing four attributes of each token: text, pos_, dep_, and lemma_.\n",
        "\n",
        "--------------------------------\n",
        "Let's break down the key concepts:\n",
        "\n",
        "##for token in sents[1]::\n",
        "\n",
        "This iterates over each token in the second sentence of the sents list.\n",
        "\n",
        "The sentence sents[1] is treated as an iterable collection of tokens.\n",
        "\n",
        "The index 1 refers to the second sentence (since Python uses zero-based indexing).\n",
        "\n",
        "----------------------------------------------------\n",
        "##Token Attributes:\n",
        "\n",
        "###token.text:\n",
        "\n",
        "This is the raw text of the token (the actual word or punctuation in the sentence).\n",
        "\n",
        "-----------------------------------\n",
        "###token.pos_:\n",
        "\n",
        "This is the part-of-speech tag of the token (e.g., noun, verb, adjective).\n",
        "\n",
        "The underscore (_) indicates the string version of the tag, rather than the integer ID.\n",
        "\n",
        "-------------------------------\n",
        "##token.dep_:\n",
        "\n",
        "This is the syntactic dependency label, which describes the token's relationship to the other tokens in the sentence (e.g., subject, object, etc.).\n",
        "\n",
        "----------------------------\n",
        "##token.lemma_:\n",
        "\n",
        "The lemma is the base or dictionary form of the token (e.g., the lemma of \"running\" is \"run\").\n",
        "\n",
        "This code suggests you're likely using spaCy, a popular NLP library that assigns linguistic annotations to text.\n",
        "\n",
        "------------------------------------------\n",
        "#Example using spaCy:\n",
        "\n",
        "    import spacy\n",
        "\n",
        "# Load the English model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process some text to create the doc object\n",
        "    doc = nlp(\"This is the first sentence. This is the second sentence.\")\n",
        "\n",
        "# Extract the sentences\n",
        "    sents = [sent for sent in doc.sents]\n",
        "\n",
        "# Iterate through tokens in the second sentence and print their attributes\n",
        "    for token in sents[1]:\n",
        "       print(token.text, token.pos_, token.dep_, token.lemma_)\n",
        "\n",
        "----------------\n",
        "#Output Example:\n",
        "For the second sentence, \"This is the second sentence.\", the output might look like:\n",
        "\n",
        "    This DET nsubj this\n",
        "    is AUX ROOT be\n",
        "    the DET det the\n",
        "    second ADJ amod second\n",
        "    sentence NOUN attr sentence\n",
        "    . PUNCT punct .\n",
        "\n",
        "##DET (Determiner):\n",
        "\n",
        "Part-of-speech tag for words like \"this\" or \"the\".\n",
        "\n",
        "##AUX (Auxiliary): Verbs like \"is\".\n",
        "\n",
        "##ROOT:\n",
        "\n",
        "The main verb or central piece of the sentence.\n",
        "\n",
        "##nsubj:\n",
        "\n",
        "The nominal subject of the verb.\n",
        "\n",
        "The term nsubj, short for nominal subject, is a syntactic dependency label used in natural language processing (NLP) to describe the grammatical relationship between a subject and the main verb in a sentence.\n",
        "\n",
        "##What is a nominal subject?\n",
        "A nominal subject is the noun, pronoun, or noun phrase that performs the action or is the focus of the verb.\n",
        "\n",
        "It tells us who or what is doing the action of the verb.\n",
        "\n",
        "In the context of dependency parsing (e.g., in libraries like spaCy), the nsubj label is applied to the token that functions as the subject in relation to the main verb (the root of the sentence).\n",
        "\n",
        "##Example:\n",
        "Consider the sentence:\n",
        "\n",
        "    \"The cat sits on the mat.\"\n",
        "\n",
        "Here is the dependency breakdown:\n",
        "\n",
        "\"The cat\" is the nominal subject because it is the noun phrase that performs the action (sitting).\n",
        "\n",
        "\"sits\" is the verb.\n",
        "\n",
        "In a dependency parse, the word \"cat\" would be labeled as nsubj to indicate that it is the nominal subject of the verb\n",
        "\n",
        "\"sits.\"\n",
        "\n",
        "##Visual Breakdown:\n",
        "The cat → nsubj → sits\n",
        "\n",
        "##Another example:\n",
        "\n",
        "\"She is reading a book.\"\n",
        "\n",
        "\"She\" is the nominal subject because \"she\" is doing the action (reading).\n",
        "\n",
        "\"is reading\" is the verb phrase.\n",
        "In this case:\n",
        "\n",
        "\"She\" will be tagged as the nsubj in the dependency parse because it is the subject of the sentence.\n",
        "\n",
        "nsubj labels the subject of the sentence, usually a noun or pronoun, that performs the action of the main verb.\n",
        "\n",
        "##lemma_:\n",
        "\n",
        "The base form of the token (e.g., \"is\" -> \"be\").\n",
        "\n",
        "Each token's attributes provide rich linguistic information about the sentence structure."
      ],
      "metadata": {
        "id": "xhaL24Q_JPal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O84Y9O0XAHYc",
        "outputId": "e87f9827-6eb8-4245-e3a8-e94612b2e98f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The             DET   det        the            \n",
            "man             NOUN  poss       man            \n",
            "'s              PART  case       's             \n",
            "hands           NOUN  nsubj      hand           \n",
            "were            AUX   ROOT       be             \n",
            "behind          ADP   prep       behind         \n",
            "\n",
            "               SPACE dep        \n",
            "              \n",
            "his             PRON  poss       his            \n",
            "back            NOUN  pobj       back           \n",
            ",               PUNCT punct      ,              \n",
            "the             DET   det        the            \n",
            "wrists          NOUN  appos      wrist          \n",
            "bound           VERB  acl        bind           \n",
            "with            ADP   prep       with           \n",
            "a               DET   det        a              \n",
            "cord            NOUN  pobj       cord           \n",
            ".               PUNCT punct      .              \n",
            "                SPACE dep                       \n"
          ]
        }
      ],
      "source": [
        "# CHALLENGE SOLUTION:\n",
        "for token in sents[1]:\n",
        "    print(f'{token.text:{15}} {token.pos_:{5}} {token.dep_:{10}} {token.lemma_:{15}}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining the: bound           VERB  acl        bind\n",
        "This line represents information about the word \"bound\" in a sentence, specifically its linguistic properties as extracted from a natural language processing (NLP) parser, such as spaCy.\n",
        "\n",
        "---------------------------------------\n",
        "Here’s what each part of the line means:\n",
        "\n",
        "##1. bound (the word itself)\n",
        "This is the actual word (or token) being analyzed, in this case, \"bound.\"\n",
        "\n",
        "##2. VERB (part-of-speech tag)\n",
        "VERB is the part of speech assigned to the word \"bound,\" indicating that it is being used as a verb in this context.\n",
        "\n",
        "The word \"bound\" can also function as an adjective or noun (e.g., \"He is bound to the task\" or \"She is homeward bound\"), but here it is identified as a verb (e.g., \"He bound the book\").\n",
        "\n",
        "##3. acl (dependency label: adjectival clause)\n",
        "acl stands for adjectival clause (or relative clause).\n",
        "\n",
        "It is a syntactic dependency label indicating that the verb \"bound\" is part of a clause that describes a noun.\n",
        "\n",
        "An adjectival clause (or adjective clause) functions like an adjective, giving more information about a noun.\n",
        "\n",
        "When the verb is part of a clause that modifies a noun, it receives the acl label.\n",
        "\n",
        "--------------------------------\n",
        "#Example:\n",
        "\n",
        "\"The book bound in leather is expensive.\"\n",
        "\n",
        "In this sentence, \"bound in leather\" is an adjectival clause modifying the noun \"book.\"\n",
        "\n",
        "\"bound\" is a verb in the clause that provides more information about the \"book.\"\n",
        "\n",
        "So, \"bound\" would get the acl dependency label because it is acting in an adjectival clause.\n",
        "\n",
        "#4. bind (lemma)\n",
        "Lemma is the base or dictionary form of a word. The lemma for \"bound\" is \"bind.\"\n",
        "\n",
        "Lemmatization reduces the word to its root form, so here \"bound\" is transformed back to its base form \"bind.\"\n",
        "\n",
        "This helps in understanding the core meaning of the word regardless of its inflected form (e.g., \"running\" → \"run\", \"bound\" → \"bind\").\n",
        "\n",
        "#Putting it all together:\n",
        "The word \"bound\" is identified as a verb (VERB).\n",
        "\n",
        "It functions in an adjectival clause (acl), meaning it modifies a noun by describing or adding information about it.\n",
        "\n",
        "Its lemma is \"bind\", which is the base form of the word \"bound.\"\n",
        "##Example Sentence:\n",
        "In a sentence like:\n",
        "\n",
        "\"The document bound by the lawyer was important.\"\n",
        "Here:\n",
        "\n",
        "\"bound\" is the verb (part of an adjectival clause modifying \"document\").\n",
        "\n",
        "The dependency acl indicates that \"bound by the lawyer\" describes the noun \"document.\"\n",
        "\n",
        "The lemma for \"bound\" is \"bind.\""
      ],
      "metadata": {
        "id": "gNRSX3-QRKxT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhrzn7GtAHYd"
      },
      "source": [
        "**6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text**<br>\n",
        "HINT: You should include an `'IS_SPACE': True` pattern between the two words!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pGVSWQqqAHYd"
      },
      "outputs": [],
      "source": [
        "# Import the Matcher library:\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ]
    },
    {
      "source": [
        "# Create a pattern and add it to matcher:\n",
        "\n",
        "pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP':'*'}, {'LOWER': 'vigorously'}]\n",
        "\n",
        "matcher.add('Swimming', [pattern]) # Removing the None argument and enclosing the pattern in a list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "edrtsgwBU_vP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating pattern with Spacy\n",
        "This code snippet uses spaCy's Matcher, a tool for finding specific patterns in a text.\n",
        "\n",
        "-------------------------\n",
        "Let's break down each part to understand what's happening:\n",
        "\n",
        "##1. Creating a pattern:\n",
        "    pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP':'*'}, {'LOWER': 'vigorously'}]\n",
        "\n",
        "This defines a pattern that will be used to match sequences of tokens in a text. Here's how this pattern works:\n",
        "\n",
        "    {'LOWER': 'swimming'}:\n",
        "\n",
        "This looks for a token with the lowercase form \"swimming\".\n",
        "\n",
        "LOWER specifies that the matching should be case-insensitive, meaning it will match \"Swimming\", \"swimming\", \"SWIMMING\", etc.\n",
        "\n",
        "    {'IS_SPACE': True, 'OP': '*'}:\n",
        "\n",
        "This matches any amount of whitespace (spaces, tabs, etc.) between \"swimming\" and \"vigorously.\"\n",
        "\n",
        "##IS_SPACE\n",
        "True means the pattern looks for space characters.\n",
        "\n",
        "##OP: '*'\n",
        "is an operator that means \"zero or more\" occurrences of the previous token.\n",
        "\n",
        "So, there could be no space, one space, or multiple spaces (even none).\n",
        "\n",
        "{'LOWER': 'vigorously'}:\n",
        "\n",
        "This looks for a token with the lowercase form \"vigorously\", again case-insensitive.\n",
        "\n",
        "---------------------------\n",
        "##2. Adding the pattern to the matcher:\n",
        "\n",
        "    matcher.add('Swimming', [pattern])\n",
        "\n",
        "###matcher.add():\n",
        "This adds the pattern to the matcher object in spaCy.\n",
        "\n",
        "'Swimming' is the label for the pattern.\n",
        "\n",
        "It gives the pattern a name or identifier, which will be returned when a match is found.\n",
        "\n",
        "The second argument is a list of patterns.\n",
        "\n",
        "Since there is just one pattern here, it is wrapped in a list ([pattern]).\n",
        "\n",
        "##Explanation of Change:\n",
        "##Removing the None argument:\n",
        "\n",
        "Older versions of spaCy's matcher required a third argument (None), but recent versions don’t.\n",
        "\n",
        "By removing it, the code uses the latest syntax.\n",
        "\n",
        "##Enclosing the pattern in a list:\n",
        "\n",
        "The matcher expects the second argument to be a list of patterns, even if you're adding just one pattern.\n",
        "\n",
        "So, the pattern needs to be enclosed in square brackets.\n",
        "\n",
        "----------------------\n",
        "#Summary\n",
        "The pattern is designed to match the phrase \"swimming vigorously\" with any amount of space (or no space) between the two words.\n",
        "\n",
        "After defining the pattern, it's added to the matcher with the label 'Swimming'.\n",
        "\n",
        "----------------\n",
        "##Example Usage:\n",
        "Here’s how this might work with a spaCy matcher:\n",
        "\n",
        "    import spacy\n",
        "    from spacy.matcher import Matcher\n",
        "\n",
        "# Load the English model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create the matcher\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the pattern\n",
        "    pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP': '*'}, {'LOWER': 'vigorously'}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "    matcher.add('Swimming', [pattern])\n",
        "\n",
        "# Sample text\n",
        "    doc = nlp(\"She was swimming   vigorously in the pool.\")\n",
        "\n",
        "# Apply the matcher to the doc\n",
        "    matches = matcher(doc)\n",
        "\n",
        "# Print the matches\n",
        "    for match_id, start, end in matches:\n",
        "       matched_span = doc[start:end]  # The matched span\n",
        "       print(matched_span.text)  # Output: \"swimming   vigorously\"\n",
        "\n",
        "In this example:\n",
        "\n",
        "The matcher would identify and extract the phrase \"swimming vigorously\" from the text, regardless of the number of spaces between the words."
      ],
      "metadata": {
        "id": "i8DLbLDYVpcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOOu_U4TAHYd",
        "outputId": "067a83d3-ee40-443c-cd8f-04d2f07038c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(12881893835109366681, 1274, 1277), (12881893835109366681, 3609, 3612)]\n"
          ]
        }
      ],
      "source": [
        "# Create a list of matches called \"found_matches\" and print the list:\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying the spaCy matcher to a document (doc)\n",
        "In this code snippet, you're applying the spaCy matcher to a document (doc) and storing the results in a list called found_matches, then printing it.\n",
        "\n",
        "--------------------------------------\n",
        "Let’s break down how this works:\n",
        "\n",
        "#1. found_matches = matcher(doc):\n",
        "This line applies the matcher to the document doc.\n",
        "\n",
        "The matcher(doc) function searches the document for patterns that were previously added to the matcher (e.g., the pattern for matching \"swimming vigorously\").\n",
        "\n",
        "It returns a list of matches. Each match is a tuple containing three elements:\n",
        "\n",
        "##match_id:\n",
        "An integer representing the ID of the matched pattern. This is usually derived from the string label (like 'Swimming') used when adding the pattern.\n",
        "\n",
        "##start:\n",
        "The starting index (token position) of the matched span in the document.\n",
        "\n",
        "##end:\n",
        "\n",
        "The ending index (token position) of the matched span in the document.\n",
        "\n",
        "----------------------------------\n",
        "##2. print(found_matches):\n",
        "This simply prints the list of found matches to the console. Each match in the list is a tuple as described above.\n",
        "\n",
        "------------------------\n",
        "#Example:\n",
        "Let’s assume that you have already added the pattern for matching the phrase \"swimming vigorously\" as shown in your previous example.\n",
        "\n",
        "Here’s what it would look like when using this snippet:\n",
        "\n",
        "-----------------------------------\n",
        "#Import spacy and the Matcher class\n",
        "    import spacy\n",
        "    from spacy.matcher import Matcher\n",
        "\n",
        "# Load the spaCy English model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create the matcher object\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern for \"swimming vigorously\"\n",
        "    pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP': '*'}, {'LOWER': 'vigorously'}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "    matcher.add('Swimming', [pattern])\n",
        "\n",
        "# Process some text\n",
        "    doc = nlp(\"She was swimming vigorously in the pool. They were swimming   vigorously as well.\")\n",
        "\n",
        "# Apply the matcher to the doc and store the matches\n",
        "    found_matches = matcher(doc)\n",
        "\n",
        "# Print the list of matches\n",
        "    print(found_matches)\n",
        "\n",
        "-----------------------------\n",
        "#Output\n",
        "The found_matches might look something like this:\n",
        "\n",
        "[(18320432761005076752, 2, 4), (18320432761005076752, 8, 10)]\n",
        "\n",
        "#Explanation of the Output:\n",
        "##18320432761005076752:\n",
        "\n",
        "This is the ID for the pattern that was matched (corresponding to the label 'Swimming' that was provided when the pattern was added to the matcher).\n",
        "\n",
        "##2, 4:\n",
        "These are the start and end token indices for the match.\n",
        "\n",
        "So in the document doc, the match spans from token at index 2 to token at index 4.\n",
        "\n",
        "You can extract the matched text using these indices.\n",
        "\n",
        "##Extracting the Matched Text:\n",
        "To get the actual text that was matched, you can use doc[start:end] for each match:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "       matched_span = doc[start:end]\n",
        "       print(matched_span.text)\n",
        "\n",
        "##Output for this:\n",
        "\n",
        "swimming vigorously\n",
        "\n",
        "swimming   vigorously\n",
        "\n",
        "This would print the exact phrases that matched the pattern.\n"
      ],
      "metadata": {
        "id": "r3nvHD-XgPu2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pDQtIQ8AHYd"
      },
      "source": [
        "**7. Print the text surrounding each found match**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQoed7fmAHYd",
        "outputId": "b57555e0-8172-46a5-ea62-dd6c3309b5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By diving I could evade the bullets and, swimming\n",
            "vigorously, reach the bank, take to the woods and get away home\n"
          ]
        }
      ],
      "source": [
        "print(doc[1265:1290])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffee5rftAHYe",
        "outputId": "f764000b-0767-455d-de5a-574e28eccc01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all this over his shoulder; he was now swimming\n",
            "vigorously with the current\n"
          ]
        }
      ],
      "source": [
        "print(doc[3600:3615])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B6rVhXkAHYe"
      },
      "source": [
        "**EXTRA CREDIT:<br>Print the *sentence* that contains each found match**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhYUk7EvAHYe",
        "outputId": "e579a470-7a87-4472-d37a-8c7701f7e114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By diving I could evade the bullets and, swimming\n",
            "vigorously, reach the bank, take to the woods and get away home.  \n"
          ]
        }
      ],
      "source": [
        "for sent in sents:\n",
        "    if found_matches[0][1] < sent.end:\n",
        "        print(sent)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXxNFfj1AHYe",
        "outputId": "c4b93465-18cf-46da-ca98-b63d3abf76fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The hunted man saw all this over his shoulder; he was now swimming\n",
            "vigorously with the current.  \n"
          ]
        }
      ],
      "source": [
        "for sent in sents:\n",
        "    if found_matches[1][1] < sent.end:\n",
        "        print(sent)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Sentence\n",
        "This snippet of code is designed to iterate through sentences (sents) and find the first sentence that contains the start of the first match in found_matches.\n",
        "\n",
        "Once that sentence is found, it prints the sentence and breaks the loop.\n",
        "\n",
        "------------------------------\n",
        "Let's break this down step by step:\n",
        "\n",
        "#1. for sent in sents:\n",
        "This loops through all the sentences in the sents list.\n",
        "sents is likely a list of sentences from a document (created using something like doc.sents in spaCy).\n",
        "\n",
        "#2. if found_matches[0][1] < sent.end:\n",
        "\n",
        "##found_matches[0][1]:\n",
        "This accesses the start index of the first match in found_matches.\n",
        "\n",
        "In spaCy, found_matches stores tuples where:\n",
        "\n",
        "The first element is the match ID.\n",
        "\n",
        "The second element is the start token index of the match.\n",
        "\n",
        "The third element is the end token index of the match.\n",
        "\n",
        "##found_matches[0][1]\n",
        "\n",
        "refers to the start index of the first match in the document.\n",
        "\n",
        "##sent.end:\n",
        "This is the end token index of the current sentence (sent).\n",
        "\n",
        "In spaCy, sent.end gives the index of the token that comes after the last token in the sentence.\n",
        "\n",
        "The condition checks whether the start of the first match (found_matches[0][1]) falls before the end of the current sentence.\n",
        "\n",
        "If it does, that means the first match occurs in this sentence.\n",
        "\n",
        "-----------------------------\n",
        "#3. print(sent):\n",
        "Once a sentence is found that contains the match, it is printed.\n",
        "\n",
        "---------------------\n",
        "#4. break:\n",
        "This breaks the loop after finding the first sentence containing the match, meaning no further sentences will be checked.\n",
        "\n",
        "--------------------------------\n",
        "#Example Code with spaCy:\n",
        "\n",
        "    import spacy\n",
        "    from spacy.matcher import Matcher\n",
        "\n",
        "# Load a small spaCy English model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create the matcher object\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern\n",
        "    pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP': '*'}, {'LOWER': 'vigorously'}]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "    matcher.add('Swimming', [pattern])\n",
        "\n",
        "# Process a document\n",
        "    doc = nlp(\"She was swimming vigorously in the pool. They were swimming   vigorously as well.\")\n",
        "\n",
        "# Extract sentences\n",
        "    sents = [sent for sent in doc.sents]\n",
        "\n",
        "# Apply the matcher\n",
        "    found_matches = matcher(doc)\n",
        "\n",
        "# Find and print the sentence containing the first match\n",
        "    for sent in sents:\n",
        "       if found_matches[0][1] < sent.end:\n",
        "         print(sent)\n",
        "         break\n",
        "##Output:\n",
        "\n",
        "\n",
        "She was swimming vigorously in the pool.\n",
        "\n",
        "#Explanation:\n",
        "sents contains the two sentences:\n",
        "\n",
        "    \"She was swimming vigorously in the pool.\"\n",
        "    \"They were swimming vigorously as well.\"\n",
        "\n",
        "found_matches[0][1] refers to the start index of the first match, which corresponds to the word \"swimming\" in the first sentence.\n",
        "\n",
        "The loop checks if the start index of the first match is less than the end of the current sentence.\n",
        "\n",
        "Since the first match occurs in the first sentence, the sentence is printed, and the loop is stopped using break.\n",
        "\n",
        "Purpose:\n",
        "This approach is useful when you want to identify the first sentence in which a specific match (e.g., a pattern like \"swimming vigorously\") appears within a larger text."
      ],
      "metadata": {
        "id": "mOAwK_boAiQj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ouNJeC0AHYe"
      },
      "source": [
        "### Great Job!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
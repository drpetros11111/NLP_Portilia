{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/NLP_Portilia/blob/NLP_Spacy_Basics_1/05_Vocabulary_and_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bi1KA6HxwZD"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iec6kRbSxwZF"
      },
      "source": [
        "# Vocabulary and Matching\n",
        "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
        "\n",
        "In this section we will identify and label specific phrases that match patterns we can define ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57oOzcb4xwZF"
      },
      "source": [
        "## Rule-based Matching\n",
        "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "biQ5TPpHxwZG"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mX3S0n5TxwZH"
      },
      "outputs": [],
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Matcher\n",
        "In this code snippet, you're importing the Matcher from spaCy and creating a matcher object using your NLP model's vocabulary (nlp.vocab).\n",
        "\n",
        "-----------------------------\n",
        "#Explanation\n",
        "\n",
        "##from spacy.matcher import Matcher:\n",
        "\n",
        "This imports the Matcher class from spaCy, which allows you to create rule-based pattern matching for identifying specific sequences of tokens (e.g., phrases, names, or other token patterns) in a text.\n",
        "\n",
        "##matcher = Matcher(nlp.vocab):\n",
        "\n",
        "This creates a Matcher object and ties it to the vocabulary (nlp.vocab) of the current NLP model (nlp).\n",
        "\n",
        "The nlp.vocab object contains all the lexical information (like words and their attributes) used by the NLP model.\n",
        "\n",
        "Once you create the matcher object, you can define specific patterns and use it to find these patterns in the text processed by the nlp object.\n",
        "\n",
        "-------------------------------\n",
        "----------------------------\n",
        "#How the Matcher Works:\n",
        "Matcher allows you to define patterns based on token attributes, such as the token's text, lemma (base form), part-of-speech tag, etc.\n",
        "\n",
        "You can then apply the matcher to a Doc object (like doc from earlier) to find all occurrences of the patterns in the text.\n",
        "\n",
        "##Example:\n",
        "###Adding and Using Patterns\n",
        "Let's say you want to find all occurrences of the phrase \"quick brown fox\" in a text. You can define a pattern for this sequence and apply the matcher.\n",
        "\n",
        "---------------------------\n",
        "# Define a pattern\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "\n",
        "--------------------------------\n",
        "# Add the pattern to the matcher\n",
        "    matcher.add(\"QUICK_BROWN_FOX\", [pattern])\n",
        "\n",
        "---------------------------\n",
        "# Apply the matcher to the processed doc\n",
        "    matches = matcher(doc)\n",
        "\n",
        "---------------------------------\n",
        "# Print the matched spans\n",
        "    for match_id, start, end in matches:\n",
        "       span = doc[start:end]  # The matched span\n",
        "       print(f\"Matched: {span.text}\")\n",
        "\n",
        "-------------------------------\n",
        "#Explanation of the Pattern:\n",
        "The pattern is a list of dictionaries, where each dictionary corresponds to a token in the sequence:\n",
        "\n",
        "{\"LOWER\": \"quick\"}: This looks for a token where the lowercase form is \"quick\".\n",
        "\n",
        "{\"LOWER\": \"brown\"}: This looks for a token where the lowercase form is \"brown\".\n",
        "\n",
        "{\"LOWER\": \"fox\"}: This looks for a token where the lowercase form is \"fox\".\n",
        "\n",
        "matcher.add(\"QUICK_BROWN_FOX\", [pattern]):\n",
        "\n",
        "Adds the pattern to the matcher with the name \"QUICK_BROWN_FOX\".\n",
        "\n",
        "The list around pattern allows you to add multiple patterns under the same name.\n",
        "\n",
        "\n",
        "##matches = matcher(doc):\n",
        "\n",
        "Applies the matcher to the doc object to find all occurrences of the pattern.\n",
        "\n",
        "matches returns a list of tuples in the format (match_id, start, end) where:\n",
        "\n",
        "match_id: The ID of the matched pattern.\n",
        "\n",
        "start: The starting token index of the match.\n",
        "\n",
        "end: The ending token index of the match.\n",
        "\n",
        "span = doc[start:end]:\n",
        "\n",
        "Extracts the matched text from the doc using the start and end indices.\n",
        "\n",
        "--------------------\n",
        "#Output:\n",
        "For the sentence \"The quick brown fox jumped over the lazy dog's back.\", the output would be:\n",
        "\n",
        "Matched: quick brown fox\n",
        "\n",
        "Would you like to explore more examples or go deeper into how pattern matching can be customized?"
      ],
      "metadata": {
        "id": "1-Q3zW3yyfz7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdEF2_gbxwZH"
      },
      "source": [
        "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7WnGOrZxwZH"
      },
      "source": [
        "### Creating patterns\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "# Pass a list of patterns as the second argument\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nnuc021i7jat"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQgNerRFxwZH"
      },
      "source": [
        "Let's break this down:\n",
        "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
        "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
        "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
        "\n",
        "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
        "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQcOfVcxwZI"
      },
      "source": [
        "### Applying the matcher to a Doc object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpRS4nyfxwZI",
        "outputId": "173520f8-447d-43c5-cdfb-ccde7bdb9d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Solar Power industry continues to grow as demand for solarpower increases. Solar-power cars are gaining popularity.\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjA4_as4xwZI",
        "outputId": "ba4a80af-f95c-43df-cc6a-d6755c67ebb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Matcher\n",
        "\n",
        "-------------------\n",
        "##found_matches = matcher(doc):\n",
        "\n",
        "matcher(doc) applies the matcher object (which contains pre-defined patterns) to the doc.\n",
        "\n",
        "The doc is a processed text, typically created by passing a string of text through nlp (the spaCy language model).\n",
        "\n",
        "This line returns a list of matches found in the document based on the patterns you defined earlier using the Matcher object.\n",
        "\n",
        "------------------------------\n",
        "##print(found_matches):\n",
        "\n",
        "This prints the matches that were found in the doc.\n",
        "\n",
        "The matches are returned in the form of a list of tuples.\n",
        "\n",
        "----------------------\n",
        "Each tuple in the list contains three values:\n",
        "\n",
        "##match_id:\n",
        "An integer or hash representing the ID of the pattern that was matched.\n",
        "\n",
        "##start:\n",
        "\n",
        "The index of the token where the match starts (inclusive).\n",
        "\n",
        "end: The index of the token where the match ends (exclusive).\n",
        "\n",
        "-------------------------\n",
        "---------------------------\n",
        "#Example of Output\n",
        "The output of found_matches is typically a list of tuples that look like this:\n",
        "\n",
        "    [(match_id, start, end), (match_id, start, end), ...]\n",
        "\n",
        "For example:\n",
        "\n",
        "    [(1234567890123456789, 2, 5), (9876543210987654321, 7, 9)]\n",
        "\n",
        "Here’s what each value in the tuple represents:\n",
        "\n",
        "##match_id:\n",
        "\n",
        "This is a unique identifier (often a hash or integer) representing the specific pattern that was matched.\n",
        "\n",
        "When adding a pattern to the Matcher, you often give it a name, and this is how spaCy internally tracks it.\n",
        "\n",
        "##start and end:\n",
        "\n",
        "These are the indices of the tokens in the doc where the match occurred.\n",
        "\n",
        "In this case:\n",
        "The first match spans the tokens from index 2 to 5.\n",
        "\n",
        "The second match spans the tokens from index 7 to 9.\n",
        "\n",
        "Extracting and Printing the Matched Text:\n",
        "You can extract the matched span from the document using the start and end indices.\n",
        "\n",
        "------------------------\n",
        "Here’s an example of how to do this:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "       matched_span = doc[start:end]  # Get the span of matched tokens\n",
        "       print(f\"Match found: {matched_span.text}\")\n",
        "\n",
        "This code will print the actual text that was matched in the document.\n",
        "\n",
        "-----------------------\n",
        "##Example Scenario\n",
        "Let’s assume you have a document \"The quick brown fox jumps over the lazy dog.\", and you added a pattern to the matcher to detect the phrase \"quick brown fox\".\n",
        "\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "    matcher.add(\"ANIMAL_ACTION\", [pattern])\n",
        "    found_matches = matcher(doc)\n",
        "    print(found_matches)\n",
        "\n",
        "------------------\n",
        "##Possible Output:\n",
        "\n",
        "  [(1234567890123456789, 1, 4)]\n",
        "This output tells you:\n",
        "\n",
        "The match ID (1234567890123456789) corresponds to the pattern \"quick brown fox\".\n",
        "\n",
        "The matched span starts at index 1 (for \"quick\") and ends at index 4 (the token after \"fox\").\n",
        "\n",
        "You can print the actual matched text:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "       print(f\"Matched: {doc[start:end].text}\")\n",
        "\n",
        "-----------------\n",
        "##Output:\n",
        "\n",
        "Matched: quick brown fox\n",
        "\n",
        "-----------------\n",
        "#In Summary\n",
        "matcher(doc) finds all patterns in the document doc.\n",
        "\n",
        "found_matches is a list of tuples, where each tuple contains the match ID, start, and end token indices of the matched span.\n",
        "\n",
        "You can use the start and end indices to extract the matched span from the doc and print the corresponding text."
      ],
      "metadata": {
        "id": "LcWBAlvj8jOM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuUnHBZgxwZJ"
      },
      "source": [
        "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdHdmjdwxwZJ",
        "outputId": "f5a70055-79e8-4a8b-84d7-004f9e6eb321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8656102463236116519 SolarPower 1 3 Solar Power\n",
            "8656102463236116519 SolarPower 10 11 solarpower\n",
            "8656102463236116519 SolarPower 13 16 Solar-power\n"
          ]
        }
      ],
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the results of pattern matching\n",
        "    for match_id, start, end in found_matches:\n",
        "       string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "       span = doc[start:end]                    # get the matched span\n",
        "       print(match_id, string_id, start, end, span.text)\n",
        "\n",
        "----------------------\n",
        "##for match_id, start, end in found_matches::\n",
        "\n",
        "This loops over all the matches returned by the matcher, where:\n",
        "\n",
        "##match_id:\n",
        "\n",
        "The unique identifier (usually a hash or integer) for the matched pattern.\n",
        "start: The index of the first token of the matched span in the doc.\n",
        "\n",
        "##end:\n",
        "\n",
        "The index of the token immediately after the last token in the matched span.\n",
        "\n",
        "##string_id = nlp.vocab.strings[match_id]:\n",
        "\n",
        "nlp.vocab.strings is a spaCy method that allows you to convert the integer match_id back into its string form (the name you gave the pattern when adding it to the matcher).\n",
        "\n",
        "This step retrieves the human-readable string name for the matched pattern.\n",
        "\n",
        "##span = doc[start:end]:\n",
        "\n",
        "doc[start:end] creates a span from the doc, which includes all tokens between start and end indices.\n",
        "\n",
        "This span represents the portion of the document where the pattern was found.\n",
        "print(match_id, string_id, start, end, span.text):\n",
        "\n",
        "-----------------\n",
        "##This prints the following:\n",
        "match_id: The integer ID of the match.\n",
        "\n",
        "string_id: The string name corresponding to the match (retrieved from nlp.vocab.strings).\n",
        "\n",
        "start: The starting token index of the matched span.\n",
        "\n",
        "end: The ending token index (exclusive) of the matched span.\n",
        "\n",
        "span.text: The actual text of the matched span.\n",
        "\n",
        "------------------------\n",
        "#Example Scenario:\n",
        "Let’s assume we have a document with the sentence \"The quick brown fox jumps over the lazy dog.\" and a matcher that looks for the pattern \"quick brown fox\".\n",
        "\n",
        "Here’s what might happen step-by-step:\n",
        "\n",
        "##1. You define a pattern and add it to the matcher:\n",
        "\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "    matcher.add(\"QUICK_BROWN_FOX_PATTERN\", [pattern])\n",
        "\n",
        "##2. You process the document and apply the matcher:\n",
        "\n",
        "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "\n",
        "    found_matches = matcher(doc)\n",
        "\n",
        "##3. Now, when you run the loop:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "      string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "      span = doc[start:end]                    # get the matched span\n",
        "      print(match_id, string_id, start, end, span.text)\n",
        "      \n",
        "##4. Expected Output:\n",
        "The output would look like this (depending on your pattern match):\n",
        "\n",
        "1234567890123456789 QUICK_BROWN_FOX_PATTERN 1 4 quick brown fox\n",
        "\n",
        "Here’s what each part of the output means:\n",
        "\n",
        "##1234567890123456789:\n",
        "\n",
        "The internal match ID (a hash or integer representing the pattern).\n",
        "\n",
        "##QUICK_BROWN_FOX_PATTERN:\n",
        "\n",
        "The string name associated with the pattern that was matched.\n",
        "\n",
        "The starting index of the match (the token \"quick\" is at index 1).\n",
        "\n",
        "4: The ending index of the match (this is the index after \"fox\", so the match spans tokens 1 to 3).\n",
        "\n",
        "quick brown fox: The text of the matched span from the document.\n",
        "\n",
        "-------------------\n",
        "#Summary\n",
        "This loop extracts and prints details about each matched pattern:\n",
        "\n",
        "The internal match ID.\n",
        "The name of the pattern (e.g., \"QUICK_BROWN_FOX_PATTERN\").\n",
        "\n",
        "The starting and ending token indices.\n",
        "The actual matched text from the document.\n",
        "\n",
        "This code is useful for analyzing the results of pattern matching and understanding which patterns were matched and where they occurred in the text."
      ],
      "metadata": {
        "id": "bNUVisPCHYid"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8GnJPGvxwZJ"
      },
      "source": [
        "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRWP8S1YxwZJ"
      },
      "source": [
        "### Setting pattern options and quantifiers\n",
        "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
      ]
    },
    {
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the matcher with the shared vocabulary\n",
        "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
        "\n",
        "# Define the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2]) # Only two arguments are needed\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher in a list:\n",
        "matcher.add('SolarPower', [pattern1, pattern2])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WS-nnZ-BbxZb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain pattern2\n",
        "    pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "This line of code defines a pattern for the spaCy matcher. Let's break down what each part means:\n",
        "\n",
        "--------------------------\n",
        "##pattern2 = [...]:\n",
        "\n",
        "This assigns a list to the variable pattern2.\n",
        "\n",
        "This list contains the pattern to be matched.\n",
        "\n",
        "---------------------\n",
        "##{'LOWER': 'solar'}:\n",
        "\n",
        "This is the first item in the pattern.\n",
        "\n",
        "It's a dictionary specifying that the matcher should look for a token whose lowercase form is \"solar\".\n",
        "\n",
        "-------------------------\n",
        "##{'IS_PUNCT': True, 'OP':'*'}:\n",
        "\n",
        "This is the second item.\n",
        "\n",
        "It's also a dictionary, and it specifies that the matcher should look for any punctuation mark.\n",
        "\n",
        "The 'OP': '*' part means that this token can occur zero or more times.\n",
        "\n",
        "This allows for cases like \"solarpower\", \"solar-power\", and \"solar---power\".\n",
        "\n",
        "----------------------\n",
        "##{'LOWER': 'power'}:\n",
        "\n",
        "This is the third item, and it's similar to the first. It specifies that the matcher should look for a token whose lowercase form is \"power\".\n",
        "\n",
        "-----------------------\n",
        "#In summary\n",
        "this pattern will match any occurrence of \"solar\" followed by zero or more punctuation marks and then \"power\".\n",
        "\n",
        "This allows for flexibility in how the words \"solar\" and \"power\" are combined, such as with or without hyphens or other punctuation."
      ],
      "metadata": {
        "id": "ZKAJcrSdcOcZ"
      }
    },
    {
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2]) # Pass a list of patterns as the second argument."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QFwGYCpMbRVO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NLhu5ZwxwZK",
        "outputId": "f67ee9ab-c68c-4c44-8d77-31e4646db469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9bIsxFxwZK"
      },
      "source": [
        "This found both two-word patterns, with and without the hyphen!\n",
        "\n",
        "The following quantifiers can be passed to the `'OP'` key:\n",
        "<table><tr><th>OP</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcnXZXZuxwZK"
      },
      "source": [
        "### Be careful with lemmas!\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
      ]
    },
    {
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
        "\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}]\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2])\n",
        "\n",
        "# Create a sample document\n",
        "doc = nlp(\"Solarpower is awesome! The solar-powered car is eco-friendly.\")\n",
        "\n",
        "# Match the patterns in the document\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkh35RyJeHCk",
        "outputId": "380cc343-bf16-4f98-8994-ab31bed3c2f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 1), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BIK8vyvcxwZK"
      },
      "outputs": [],
      "source": [
        "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N80Q9p_GxwZK",
        "outputId": "f493b5bc-c8bc-42d3-fb11-e122a7c58907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqk8Mv3xwZL"
      },
      "source": [
        "<font color=green>The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "o7X-zuUZxwZL",
        "outputId": "a0fd0150-5377-4ab8-8ba4-8b27d48745f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E175] Can't remove rule for unknown match pattern ID: SolarPower",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-65f9af715eac>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Remove the old patterns to avoid duplication:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SolarPower'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add the new set of patterns to the 'SolarPower' matcher:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/matcher/matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.remove\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E175] Can't remove rule for unknown match pattern ID: SolarPower"
          ]
        }
      ],
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3, pattern4]) # Pass all patterns as a single list"
      ]
    },
    {
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
        "\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3, pattern4]) # Pass all patterns as a single list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4DtqjRxRgh-Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "-----------------------\n",
        "##matcher.add(...):\n",
        "\n",
        "##matcher:\n",
        "This is the Matcher object that you created earlier using Matcher(nlp.vocab).\n",
        "\n",
        "It is used to define and find token-based patterns in the text.\n",
        "\n",
        "##.add(...):\n",
        "\n",
        "This method is used to add a new pattern or set of patterns to the Matcher object. It associates the pattern(s) with a name (in this case, \"ANIMAL_PATTERN\"), which helps in identifying the matched patterns later.\n",
        "\n",
        "-------------------------\n",
        "#\"ANIMAL_PATTERN\" (Pattern Name):\n",
        "\n",
        "The first argument \"ANIMAL_PATTERN\" is a string that serves as a label or name for this particular pattern.\n",
        "\n",
        "When the pattern is matched in the document, this name will be associated with the match.\n",
        "\n",
        "You can give any name that makes sense for the pattern you are adding (for example, \"ANIMAL_PATTERN\" to indicate that the pattern matches animal-related phrases).\n",
        "\n",
        "##[pattern] (Pattern List):\n",
        "\n",
        "The second argument is a list of patterns that you want to match.\n",
        "\n",
        "Each pattern is defined as a list of dictionaries, where each dictionary specifies the attributes of a token in the pattern.\n",
        "\n",
        "In this case, [pattern] means you are adding a single pattern.\n",
        "\n",
        "If you have multiple patterns, you can add them all at once by passing a list of them.\n",
        "\n",
        "Each pattern is a list of dictionaries, where each dictionary describes one token's attributes (like LOWER, POS, LEMMA, etc.).\n",
        "\n",
        "----------------------------\n",
        "#Example of the Pattern:\n",
        "Let’s look at an example pattern and the call to matcher.add in context.\n",
        "\n",
        "    from spacy.matcher import Matcher\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "##matcher = Matcher(nlp.vocab)\n",
        "\n",
        "----------------------------\n",
        "# Define a pattern that looks for \"quick\" followed by \"brown\" followed by \"fox\"\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "\n",
        "# Add the pattern to the matcher with the name \"ANIMAL_PATTERN\"\n",
        "    matcher.add(\"ANIMAL_PATTERN\", [pattern])\n",
        "\n",
        "# Now apply the matcher to a document\n",
        "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "       matched_span = doc[start:end]\n",
        "       print(f\"Match ID: {match_id}, Pattern Name: {nlp.vocab.strings[match_id]}, Matched Text: {matched_span.text}\")\n",
        "\n",
        "---------------------------\n",
        "#Detailed Breakdown:\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "\n",
        "This is the pattern being added to the matcher.\n",
        "\n",
        "It is a list of three dictionaries, where each dictionary describes one token.\n",
        "\n",
        "------------\n",
        "    {\"LOWER\": \"quick\"}\n",
        "\n",
        "This specifies that the first token should be the lowercase word \"quick\".\n",
        "\n",
        "{\"LOWER\": \"brown\"}: The second token should be the lowercase word \"brown\".\n",
        "\n",
        "{\"LOWER\": \"fox\"}: The third token should be the lowercase word \"fox\".\n",
        "\n",
        "The pattern matches any sequence of tokens that follow the exact order \"quick\", \"brown\", and \"fox\".\n",
        "matcher.add(\"ANIMAL_PATTERN\", [pattern]):\n",
        "\n",
        "The add method adds this pattern to the Matcher under the name \"ANIMAL_PATTERN\".\n",
        "\n",
        "You can now search for this pattern in any document processed by nlp.\n",
        "\n",
        "---------------------------\n",
        "#Output Example:\n",
        "If the document \"The quick brown fox jumps over the lazy dog.\" is processed, the matcher will find a match because the sequence \"quick brown fox\" exists in the document. The output would be:\n",
        "\n",
        "##Match ID: 1234567890123456789, Pattern ##Name: ANIMAL_PATTERN, Matched Text: quick brown fox\n",
        "\n",
        "-----------------\n",
        "#What Happens in the Code:\n",
        "You define a pattern that looks for a specific sequence of tokens (in this case, \"quick\", \"brown\", \"fox\").\n",
        "\n",
        "You add this pattern to the Matcher under the name \"ANIMAL_PATTERN\".\n",
        "You apply the matcher to a doc object (the document).\n",
        "\n",
        "If the pattern is found in the document, it returns a match. You can then print the matched text and the name of the pattern that matched it.\n",
        "\n",
        "----------\n",
        "#Summary\n",
        "matcher.add(\"ANIMAL_PATTERN\", [pattern]): This line adds a pattern to the Matcher object with the label \"ANIMAL_PATTERN\".\n",
        "\n",
        "The pattern is a list of dictionaries, where each dictionary defines the properties of one token (such as its text, part of speech, etc.).\n",
        "\n",
        "Once the pattern is added, you can use the matcher to find occurrences of that pattern in any document and retrieve the matched spans.\n",
        "\n",
        "Would you like further details on defining more complex patterns or token attributes?\n",
        "\n",
        "-------------------------\n",
        "-------------------------\n",
        "#Note:\n",
        "\n",
        "------------------\n",
        "# Add the pattern to the matcher with the name \"ANIMAL_PATTERN\"\n",
        "    matcher.add(\"ANIMAL_PATTERN\", [pattern])\n",
        "\n",
        "-----------------------------\n",
        "#1. The Pattern Definition:\n",
        "\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "\n",
        "Here, we are defining a single pattern.\n",
        "\n",
        "This pattern is a list of dictionaries, where each dictionary specifies the attributes of a single token.\n",
        "\n",
        "In this example:\n",
        "\n",
        "{\"LOWER\": \"quick\"}: This dictionary specifies that the first token should be the word \"quick\" in lowercase.\n",
        "\n",
        "{\"LOWER\": \"brown\"}: The second token should be \"brown\" in lowercase.\n",
        "\n",
        "{\"LOWER\": \"fox\"}: The third token should be \"fox\" in lowercase.\n",
        "\n",
        "This structure allows you to match the sequence of tokens \"quick brown fox\" in a document.\n",
        "\n",
        "-----------------------\n",
        "#2. Adding the Pattern to the Matcher:\n",
        "    matcher.add(\"ANIMAL_PATTERN\", [pattern])\n",
        "\n",
        "Now, you are adding this pattern to the Matcher object with the label \"ANIMAL_PATTERN\". Here’s why the syntax has two levels:\n",
        "\n",
        "\"ANIMAL_PATTERN\": This is the label or name you assign to the pattern. It helps identify the pattern when a match is found.\n",
        "\n",
        "[pattern]: This is where the list of patterns comes in.\n",
        "\n",
        "------------------------\n",
        "#Why do we need a list here?\n",
        "The matcher.add method takes a list of patterns as its second argument.\n",
        "\n",
        "This is because you might want to define multiple patterns that can match the same label.\n",
        "\n",
        "For example, if you wanted the label \"ANIMAL_PATTERN\" to match both \"quick brown fox\" and \"lazy dog\", you could define multiple patterns like this:\n",
        "\n",
        "    pattern1 = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "    pattern2 = [{\"LOWER\": \"lazy\"}, {\"LOWER\": \"dog\"}]\n",
        "\n",
        "--------------\n",
        "# Add both patterns to the matcher with the same label\n",
        "    matcher.add(\"ANIMAL_PATTERN\", [pattern1, pattern2])\n",
        "\n",
        "In this case, matcher.add(\"ANIMAL_PATTERN\", [pattern1, pattern2]) adds both patterns under the same label \"ANIMAL_PATTERN\".\n",
        "\n",
        "So, the matcher will search for both \"quick brown fox\" and \"lazy dog\" in the text.\n",
        "\n",
        "#Simplifying:\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]:\n",
        "    \n",
        "This defines one pattern.\n",
        "\n",
        "##[pattern]:\n",
        "\n",
        "Since the matcher.add() expects a list of patterns, we pass the single pattern in a list ([pattern]).\n",
        "\n",
        "If you had multiple patterns to match under the same label, you would define each pattern separately and pass them as a list:\n",
        "\n",
        "    matcher.add(\"PATTERN_NAME\", [pattern1, pattern2, pattern3])\n",
        "\n",
        "-------------------\n",
        "#Summary\n",
        "pattern = [...]: Defines a single pattern as a list of token attributes (in this case, lowercase words).\n",
        "\n",
        "matcher.add(\"LABEL\", [pattern]): Adds a list of one or more patterns to the matcher, with \"LABEL\" as the name for those patterns.\n",
        "\n",
        "[pattern]: Even if you have only one pattern, you need to wrap it in a list because matcher.add() expects a list of patterns.\n",
        "\n",
        "If you have multiple patterns, you can pass a list like [pattern1, pattern2, ...] to match different token sequences under the same label.\n",
        "\n",
        "Does this clarify why both syntaxes are used? Let me know if you need more examples or further clarification!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uoQ3ObsTqayg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAUuRQDNxwZL",
        "outputId": "0f613687-0d2d-4c7a-eeab-d4fe92df22f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mna1cJQexwZL"
      },
      "source": [
        "## Other token attributes\n",
        "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnWcWzzoxwZL"
      },
      "source": [
        "### Token wildcard\n",
        "You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",
        ">`[{'ORTH': '#'}, {}]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG__3Lj8xwZL"
      },
      "source": [
        "___\n",
        "## PhraseMatcher\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead.\n",
        "\n",
        "-------------------------\n",
        "#Pattern vs. Phrase Matching\n",
        "The Matcher and PhraseMatcher in spaCy are both used for finding patterns in text, but they work in fundamentally different ways and are designed for different types of tasks. Here’s a detailed comparison to help clarify the differences:\n",
        "\n",
        "--------------------\n",
        "#1. Pattern Matcher (Matcher)\n",
        "The Matcher is a flexible, token-based pattern matcher that allows you to define rules or patterns to match specific combinations of tokens.\n",
        "\n",
        "It works on token attributes, allowing for highly customizable and complex matching rules.\n",
        "\n",
        "----------------------\n",
        "#Key Characteristics:\n",
        "##Token-based:\n",
        "\n",
        "It operates on individual tokens (words) and allows you to match specific token attributes, such as text, part-of-speech tags, lemmas, etc.\n",
        "\n",
        "##Flexible Patterns:\n",
        "You can define complex patterns that include multiple tokens, optional tokens, and conditions based on token attributes.\n",
        "\n",
        "#@Customizable:\n",
        "It lets you match tokens based on various attributes like LOWER, LEMMA, POS, TAG, etc.\n",
        "\n",
        "For example, you can create a pattern that matches a verb followed by a noun, or a sequence of specific words in lowercase.\n",
        "\n",
        "Fine-Grained Control: You can define how each token in a sequence should behave.\n",
        "\n",
        "------------------------\n",
        "#Example Usage:\n",
        "\n",
        "    from spacy.matcher import Matcher\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern: match \"quick\" followed by \"brown\" followed by \"fox\"\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "    matcher.add(\"ANIMAL_PATTERN\", [pattern])\n",
        "\n",
        "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "       span = doc[start:end]\n",
        "       print(f\"Matched span: {span.text}\")\n",
        "\n",
        "---------------------------\n",
        "#2. Phrase Matcher (PhraseMatcher)\n",
        "The PhraseMatcher is designed for fast and efficient matching of large sets of exact phrases.\n",
        "\n",
        "Instead of matching token attributes or more abstract patterns, it directly matches sequences of tokens (phrases) that are pre-defined.\n",
        "\n",
        "------------------------\n",
        "##Key Characteristics:\n",
        "###Phrase-based:\n",
        "\n",
        "It matches exact sequences of tokens (phrases) that you have predefined. It doesn't work with token attributes like POS or LEMMA, but instead looks for exact phrase matches.\n",
        "\n",
        "Faster and more efficient: Since it works on exact phrases and doesn't need to evaluate token attributes, it is much faster, especially when you need to match a large list of phrases.\n",
        "\n",
        "###Ideal for Named Entities and Keywords:\n",
        "\n",
        "It’s particularly useful when you have a predefined set of phrases or entities you want to search for in the text (e.g., company names, product names, or other proper nouns).\n",
        "\n",
        "----------------------------\n",
        "#Example Usage:\n",
        "\n",
        "    from spacy.matcher import PhraseMatcher\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# Define a list of phrases to match\n",
        "    terms = [\"quick brown fox\", \"lazy dog\"]\n",
        "    patterns = [nlp(term) for term in terms]\n",
        "    phrase_matcher.add(\"ANIMAL_PHRASE\", patterns)\n",
        "\n",
        "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "    matches = phrase_matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "      span = doc[start:end]\n",
        "      print(f\"Matched phrase: {span.text}\")\n",
        "\n",
        "----------------------------\n",
        "##Key Differences Between Matcher and PhraseMatcher\n",
        "###Feature\tMatcher\tPhraseMatcher\n",
        "Matching Type\tToken-based pattern matching\tExact phrase matching\n",
        "\n",
        "Customization\tHighly customizable, token attributes like POS, LEMMA, TAG, DEP, etc.\n",
        "\n",
        "Matches exact token sequences (phrases)\n",
        "\n",
        "Flexibility\tCan define complex patterns with token attributes and logical operators (e.g., optional tokens)\tMatches predefined exact phrases only\n",
        "\n",
        "Use Cases\tMatching based on grammatical structure, token attributes, or complex rules\tFinding specific named entities or predefined keywords\n",
        "\n",
        "Efficiency\tMore flexible, but slower for large sets of phrases\tVery fast and efficient for large sets of phrases\n",
        "\n",
        "Typical Use\tGrammatical patterns, custom rules, token sequences\tNamed entity recognition, keyword extraction\n",
        "\n",
        "--------------------------------\n",
        "#When to Use Matcher:\n",
        "You need flexibility to match tokens based on attributes like part of speech, lemma, or specific sequences of tokens.\n",
        "\n",
        "You want to match grammatical structures, token types, or sequences with optional tokens.\n",
        "\n",
        "You’re dealing with complex token combinations or patterns.\n",
        "\n",
        "----------------\n",
        "#When to Use PhraseMatcher:\n",
        "You want to match exact phrases (e.g., named entities, product names, specific keywords).\n",
        "\n",
        "You have a large set of predefined phrases to look for and need a fast solution.\n",
        "\n",
        "You don’t need token attributes like POS or LEMMA, just exact token matches.\n",
        "\n",
        "---------------------------\n",
        "#Summary\n",
        "##Matcher:\n",
        "Use this for more complex, flexible patterns that involve specific token attributes.\n",
        "\n",
        "It gives you granular control over token matching but is slower for large sets of phrases.\n",
        "\n",
        "##PhraseMatcher:\n",
        "Use this for fast, exact matching of predefined phrases.\n",
        "\n",
        "It's ideal for entity recognition or finding specific phrases in text and is much more efficient when working with large sets of phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "P-dzrmYBxwZL"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JHHZALcOxwZM"
      },
      "outputs": [],
      "source": [
        "# Import the PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJyacBMxwZM"
      },
      "source": [
        "For this exercise we're going to import a Wikipedia article on *Reaganomics*<br>\n",
        "Source: https://en.wikipedia.org/wiki/Reaganomics"
      ]
    },
    {
      "source": [
        "# Import the request library\n",
        "import requests\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://en.wikipedia.org/wiki/Reaganomics'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    doc3 = nlp(response.text)\n",
        "else:\n",
        "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dVQRspqniLhi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve data\n",
        "\n",
        "-----------------------\n",
        "# 1. Checking the Response Status Code (response.status_code == 200):\n",
        "response.status_code: When making an HTTP request using libraries like requests, response is an object that holds the server's response.\n",
        "\n",
        "status_code: This attribute tells you the HTTP status code of the response. HTTP status codes indicate whether the request was successful or not.\n",
        "\n",
        "The most common codes include:\n",
        "200: OK (the request was successful, and the server returned the expected data).\n",
        "\n",
        "404: Not Found (the server couldn't find what was requested).\n",
        "\n",
        "500: Internal Server Error (the server encountered an error).\n",
        "if response.status_code == 200:: This line checks if the response from the server was successful, meaning the server returned a 200 OK status code. If so, the following block of code will execute.\n",
        "\n",
        "---------------------------\n",
        "#2. Parsing the HTML Content:\n",
        "    doc3 = nlp(response.text)\n",
        "##response.text:\n",
        "\n",
        "This retrieves the content of the HTTP response as a string (typically the raw HTML or JSON data that the server returned).\n",
        "\n",
        "##doc3 = nlp(response.text):\n",
        "\n",
        "This line uses spaCy's NLP model (nlp) to process the text content of the response. It essentially creates a Doc object from the text, allowing you to perform various NLP tasks (like tokenization, named entity recognition, etc.).\n",
        "\n",
        "###doc3:\n",
        "\n",
        "This is the result of processing the response.text through spaCy's NLP pipeline. It contains the parsed document that spaCy can work with (tokens, sentences, etc.).\n",
        "\n",
        "However, this code might seem a bit odd because normally you would not pass raw HTML to spaCy directly.\n",
        "\n",
        "If the response.text contains HTML (as most web content does), it is better to clean and extract meaningful text from the HTML using a library like BeautifulSoup before passing it to spaCy.\n",
        "\n",
        "Normally, it would look more like this:\n",
        "\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract the text from the HTML\n",
        "        clean_text = soup.get_text()\n",
        "        # Pass the clean text to spaCy for NLP processing\n",
        "        doc3 = nlp(clean_text)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
        "\n",
        "Here, BeautifulSoup is used to clean up the raw HTML and extract the actual text content that you’re interested in before passing it to spaCy.\n",
        "\n",
        "---------------------\n",
        "#3. Else Block:\n",
        "\n",
        "    else:\n",
        "       print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
        "\n",
        "If the status_code is not 200 (indicating the request was unsuccessful), the program will execute the else block and print an error message along with the actual status_code.\n",
        "\n",
        "------------------------\n",
        "#Summary\n",
        "The code checks if the HTTP request was successful by verifying if the status code is 200.\n",
        "\n",
        "If successful, it processes the response's text content using spaCy (doc3 = nlp(response.text)), though this is unconventional because HTML should typically be cleaned first.\n",
        "\n",
        "If unsuccessful, it prints an error message showing the status code of the failed request."
      ],
      "metadata": {
        "id": "qwbmV8vb4Bac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KqyyOszfxwZM"
      },
      "outputs": [],
      "source": [
        "# First, create a list of match phrases:\n",
        "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
        "\n",
        "# Next, convert each phrase to a Doc object:\n",
        "phrase_patterns = [nlp(text) for text in phrase_list]\n",
        "\n",
        "# Pass each Doc object into matcher (note the use of the asterisk!):\n",
        "matcher.add('VoodooEconomics', None, *phrase_patterns)\n",
        "\n",
        "# Build a list of matches:\n",
        "matches = matcher(doc3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtkGrxaSxwZM",
        "outputId": "be05f0b7-e9a5-4a02-ede5-16ef66fe8660"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3473369816841043438, 19502, 19504)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# (match_id, start, end)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6q0ByqMxwZM"
      },
      "source": [
        "<font color=green>The first four matches are where these terms are used in the definition of Reaganomics:</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e437qbLmxwZN",
        "outputId": "b837647f-1b4c-45e1-c888-d5b78d5ce765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<!DOCTYPE html>\n",
              "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "doc3[:70]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJbllNCxwZN"
      },
      "source": [
        "## Viewing Matches\n",
        "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYMwWtR5xwZN",
        "outputId": "112b35b2-9a43-466d-ca7c-70e2f133feae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "meta property=\"og:image:width\" content=\"800\">\n",
              "<meta property=\"og:image:height\" content=\"529"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip8fdtCBxwZO",
        "outputId": "8caf447f-1839-4312-8fdc-dcb4b1aaf1f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "doc3[2975:2995]  # The sixth match starts at doc3[2985]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5EpOQjxwZO"
      },
      "source": [
        "Another way is to first apply the `sentencizer` to the Doc, then iterate through the sentences to the match point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW_wZJ0ExwZO",
        "outputId": "ee18ad95-7feb-45ff-ea05-a1cc874b4d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 447\n"
          ]
        }
      ],
      "source": [
        "# Build a list of sentences\n",
        "sents = [sent for sent in doc3.sents]\n",
        "\n",
        "# In the next section we'll see that sentences contain start and end token values:\n",
        "print(sents[0].start, sents[0].end)"
      ]
    },
    {
      "source": [
        "# Iterate over the sentence list until the sentence end value exceeds a match start value:\n",
        "for sent in sents:\n",
        "    if matches[0][1] < sent.end:  # Access the first match\n",
        "        print(sent)\n",
        "        break"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yV0Xya7uP7j",
        "outputId": "2f55b802-1e7f-47b3-b4f0-756d5811635d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These policies are characterized as <a href=\"/wiki/Supply-side_economics\" title=\"Supply-side economics\">supply-side economics</a>, <a href=\"/wiki/Trickle-down_economics\" title=\"Trickle-down economics\">trickle-down economics</a>, or \"voodoo economics\" by opponents,<sup id=\"cite_ref-Roubini-1997_5-0\" class=\"reference\"><a href=\"#cite_note-Roubini-1997-5\"><span class=\"cite-bracket\">&#91;</span>5<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-Voodoo_economics-2004_6-0\" class=\"reference\"><a href=\"#cite_note-Voodoo_economics-2004-6\"><span class=\"cite-bracket\">&#91;</span>6<span class=\"cite-bracket\">&#93;</span></a></sup> including some Republicans, while Reagan and his advocates preferred to call it <a href=\"/wiki/Free_market_economy\" class=\"mw-redirect\" title=\"Free market economy\">free-market economics</a>.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_3P5rezxwZP"
      },
      "source": [
        "For additional information visit https://spacy.io/usage/linguistic-features#section-rule-based-matching\n",
        "## Next Up: NLP Basics Assessment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/NLP_Portilia/blob/NLP_Spacy_Basics_1/05_Vocabulary_and_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bi1KA6HxwZD"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iec6kRbSxwZF"
      },
      "source": [
        "# Vocabulary and Matching\n",
        "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
        "\n",
        "In this section we will identify and label specific phrases that match patterns we can define ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57oOzcb4xwZF"
      },
      "source": [
        "## Rule-based Matching\n",
        "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "biQ5TPpHxwZG"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mX3S0n5TxwZH"
      },
      "outputs": [],
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Matcher\n",
        "In this code snippet, you're importing the Matcher from spaCy and creating a matcher object using your NLP model's vocabulary (nlp.vocab).\n",
        "\n",
        "-----------------------------\n",
        "#Explanation\n",
        "\n",
        "##from spacy.matcher import Matcher:\n",
        "\n",
        "This imports the Matcher class from spaCy, which allows you to create rule-based pattern matching for identifying specific sequences of tokens (e.g., phrases, names, or other token patterns) in a text.\n",
        "\n",
        "##matcher = Matcher(nlp.vocab):\n",
        "\n",
        "This creates a Matcher object and ties it to the vocabulary (nlp.vocab) of the current NLP model (nlp).\n",
        "\n",
        "The nlp.vocab object contains all the lexical information (like words and their attributes) used by the NLP model.\n",
        "\n",
        "Once you create the matcher object, you can define specific patterns and use it to find these patterns in the text processed by the nlp object.\n",
        "\n",
        "-------------------------------\n",
        "----------------------------\n",
        "#How the Matcher Works:\n",
        "Matcher allows you to define patterns based on token attributes, such as the token's text, lemma (base form), part-of-speech tag, etc.\n",
        "\n",
        "You can then apply the matcher to a Doc object (like doc from earlier) to find all occurrences of the patterns in the text.\n",
        "\n",
        "##Example:\n",
        "###Adding and Using Patterns\n",
        "Let's say you want to find all occurrences of the phrase \"quick brown fox\" in a text. You can define a pattern for this sequence and apply the matcher.\n",
        "\n",
        "---------------------------\n",
        "# Define a pattern\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "\n",
        "--------------------------------\n",
        "# Add the pattern to the matcher\n",
        "    matcher.add(\"QUICK_BROWN_FOX\", [pattern])\n",
        "\n",
        "---------------------------\n",
        "# Apply the matcher to the processed doc\n",
        "    matches = matcher(doc)\n",
        "\n",
        "---------------------------------\n",
        "# Print the matched spans\n",
        "    for match_id, start, end in matches:\n",
        "       span = doc[start:end]  # The matched span\n",
        "       print(f\"Matched: {span.text}\")\n",
        "\n",
        "-------------------------------\n",
        "#Explanation of the Pattern:\n",
        "The pattern is a list of dictionaries, where each dictionary corresponds to a token in the sequence:\n",
        "\n",
        "{\"LOWER\": \"quick\"}: This looks for a token where the lowercase form is \"quick\".\n",
        "\n",
        "{\"LOWER\": \"brown\"}: This looks for a token where the lowercase form is \"brown\".\n",
        "\n",
        "{\"LOWER\": \"fox\"}: This looks for a token where the lowercase form is \"fox\".\n",
        "\n",
        "matcher.add(\"QUICK_BROWN_FOX\", [pattern]):\n",
        "\n",
        "Adds the pattern to the matcher with the name \"QUICK_BROWN_FOX\".\n",
        "\n",
        "The list around pattern allows you to add multiple patterns under the same name.\n",
        "\n",
        "\n",
        "##matches = matcher(doc):\n",
        "\n",
        "Applies the matcher to the doc object to find all occurrences of the pattern.\n",
        "\n",
        "matches returns a list of tuples in the format (match_id, start, end) where:\n",
        "\n",
        "match_id: The ID of the matched pattern.\n",
        "\n",
        "start: The starting token index of the match.\n",
        "\n",
        "end: The ending token index of the match.\n",
        "\n",
        "span = doc[start:end]:\n",
        "\n",
        "Extracts the matched text from the doc using the start and end indices.\n",
        "\n",
        "--------------------\n",
        "#Output:\n",
        "For the sentence \"The quick brown fox jumped over the lazy dog's back.\", the output would be:\n",
        "\n",
        "Matched: quick brown fox\n",
        "\n",
        "Would you like to explore more examples or go deeper into how pattern matching can be customized?"
      ],
      "metadata": {
        "id": "1-Q3zW3yyfz7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdEF2_gbxwZH"
      },
      "source": [
        "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7WnGOrZxwZH"
      },
      "source": [
        "### Creating patterns\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "# Pass a list of patterns as the second argument\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nnuc021i7jat"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQgNerRFxwZH"
      },
      "source": [
        "Let's break this down:\n",
        "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
        "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
        "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
        "\n",
        "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
        "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQcOfVcxwZI"
      },
      "source": [
        "### Applying the matcher to a Doc object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpRS4nyfxwZI",
        "outputId": "74f46fda-8275-4ecc-beb8-186aadd570c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Solar Power industry continues to grow as demand for solarpower increases. Solar-power cars are gaining popularity.\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjA4_as4xwZI",
        "outputId": "fd6f3ffa-5626-4528-bacf-d857d5169e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Matcher\n",
        "\n",
        "-------------------\n",
        "##found_matches = matcher(doc):\n",
        "\n",
        "matcher(doc) applies the matcher object (which contains pre-defined patterns) to the doc.\n",
        "\n",
        "The doc is a processed text, typically created by passing a string of text through nlp (the spaCy language model).\n",
        "\n",
        "This line returns a list of matches found in the document based on the patterns you defined earlier using the Matcher object.\n",
        "\n",
        "------------------------------\n",
        "##print(found_matches):\n",
        "\n",
        "This prints the matches that were found in the doc.\n",
        "\n",
        "The matches are returned in the form of a list of tuples.\n",
        "\n",
        "----------------------\n",
        "Each tuple in the list contains three values:\n",
        "\n",
        "##match_id:\n",
        "An integer or hash representing the ID of the pattern that was matched.\n",
        "\n",
        "##start:\n",
        "\n",
        "The index of the token where the match starts (inclusive).\n",
        "\n",
        "end: The index of the token where the match ends (exclusive).\n",
        "\n",
        "-------------------------\n",
        "---------------------------\n",
        "#Example of Output\n",
        "The output of found_matches is typically a list of tuples that look like this:\n",
        "\n",
        "    [(match_id, start, end), (match_id, start, end), ...]\n",
        "\n",
        "For example:\n",
        "\n",
        "    [(1234567890123456789, 2, 5), (9876543210987654321, 7, 9)]\n",
        "\n",
        "Here’s what each value in the tuple represents:\n",
        "\n",
        "##match_id:\n",
        "\n",
        "This is a unique identifier (often a hash or integer) representing the specific pattern that was matched.\n",
        "\n",
        "When adding a pattern to the Matcher, you often give it a name, and this is how spaCy internally tracks it.\n",
        "\n",
        "##start and end:\n",
        "\n",
        "These are the indices of the tokens in the doc where the match occurred.\n",
        "\n",
        "In this case:\n",
        "The first match spans the tokens from index 2 to 5.\n",
        "\n",
        "The second match spans the tokens from index 7 to 9.\n",
        "\n",
        "Extracting and Printing the Matched Text:\n",
        "You can extract the matched span from the document using the start and end indices.\n",
        "\n",
        "------------------------\n",
        "Here’s an example of how to do this:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "       matched_span = doc[start:end]  # Get the span of matched tokens\n",
        "       print(f\"Match found: {matched_span.text}\")\n",
        "\n",
        "This code will print the actual text that was matched in the document.\n",
        "\n",
        "-----------------------\n",
        "##Example Scenario\n",
        "Let’s assume you have a document \"The quick brown fox jumps over the lazy dog.\", and you added a pattern to the matcher to detect the phrase \"quick brown fox\".\n",
        "\n",
        "    pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "    matcher.add(\"ANIMAL_ACTION\", [pattern])\n",
        "    found_matches = matcher(doc)\n",
        "    print(found_matches)\n",
        "\n",
        "------------------\n",
        "##Possible Output:\n",
        "\n",
        "  [(1234567890123456789, 1, 4)]\n",
        "This output tells you:\n",
        "\n",
        "The match ID (1234567890123456789) corresponds to the pattern \"quick brown fox\".\n",
        "\n",
        "The matched span starts at index 1 (for \"quick\") and ends at index 4 (the token after \"fox\").\n",
        "\n",
        "You can print the actual matched text:\n",
        "\n",
        "    for match_id, start, end in found_matches:\n",
        "       print(f\"Matched: {doc[start:end].text}\")\n",
        "\n",
        "-----------------\n",
        "##Output:\n",
        "\n",
        "Matched: quick brown fox\n",
        "\n",
        "-----------------\n",
        "#In Summary\n",
        "matcher(doc) finds all patterns in the document doc.\n",
        "\n",
        "found_matches is a list of tuples, where each tuple contains the match ID, start, and end token indices of the matched span.\n",
        "\n",
        "You can use the start and end indices to extract the matched span from the doc and print the corresponding text."
      ],
      "metadata": {
        "id": "LcWBAlvj8jOM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuUnHBZgxwZJ"
      },
      "source": [
        "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdHdmjdwxwZJ",
        "outputId": "91534dad-d7c2-4fce-8097-337e512a9cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8656102463236116519 SolarPower 1 3 Solar Power\n",
            "8656102463236116519 SolarPower 10 11 solarpower\n",
            "8656102463236116519 SolarPower 13 16 Solar-power\n"
          ]
        }
      ],
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation:\n",
        "python\n",
        "Copy code\n",
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)\n",
        "for match_id, start, end in found_matches::\n",
        "\n",
        "This loops over all the matches returned by the matcher, where:\n",
        "match_id: The unique identifier (usually a hash or integer) for the matched pattern.\n",
        "start: The index of the first token of the matched span in the doc.\n",
        "end: The index of the token immediately after the last token in the matched span.\n",
        "string_id = nlp.vocab.strings[match_id]:\n",
        "\n",
        "nlp.vocab.strings is a spaCy method that allows you to convert the integer match_id back into its string form (the name you gave the pattern when adding it to the matcher).\n",
        "This step retrieves the human-readable string name for the matched pattern.\n",
        "span = doc[start:end]:\n",
        "\n",
        "doc[start:end] creates a span from the doc, which includes all tokens between start and end indices.\n",
        "This span represents the portion of the document where the pattern was found.\n",
        "print(match_id, string_id, start, end, span.text):\n",
        "\n",
        "This prints the following:\n",
        "match_id: The integer ID of the match.\n",
        "string_id: The string name corresponding to the match (retrieved from nlp.vocab.strings).\n",
        "start: The starting token index of the matched span.\n",
        "end: The ending token index (exclusive) of the matched span.\n",
        "span.text: The actual text of the matched span.\n",
        "Example Scenario:\n",
        "Let’s assume we have a document with the sentence \"The quick brown fox jumps over the lazy dog.\" and a matcher that looks for the pattern \"quick brown fox\".\n",
        "\n",
        "Here’s what might happen step-by-step:\n",
        "\n",
        "You define a pattern and add it to the matcher:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "pattern = [{\"LOWER\": \"quick\"}, {\"LOWER\": \"brown\"}, {\"LOWER\": \"fox\"}]\n",
        "matcher.add(\"QUICK_BROWN_FOX_PATTERN\", [pattern])\n",
        "You process the document and apply the matcher:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "found_matches = matcher(doc)\n",
        "Now, when you run the loop:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)\n",
        "Expected Output:\n",
        "The output would look like this (depending on your pattern match):\n",
        "\n",
        "Copy code\n",
        "1234567890123456789 QUICK_BROWN_FOX_PATTERN 1 4 quick brown fox\n",
        "Here’s what each part of the output means:\n",
        "\n",
        "1234567890123456789: The internal match ID (a hash or integer representing the pattern).\n",
        "QUICK_BROWN_FOX_PATTERN: The string name associated with the pattern that was matched.\n",
        "1: The starting index of the match (the token \"quick\" is at index 1).\n",
        "4: The ending index of the match (this is the index after \"fox\", so the match spans tokens 1 to 3).\n",
        "quick brown fox: The text of the matched span from the document.\n",
        "Summary:\n",
        "This loop extracts and prints details about each matched pattern:\n",
        "The internal match ID.\n",
        "The name of the pattern (e.g., \"QUICK_BROWN_FOX_PATTERN\").\n",
        "The starting and ending token indices.\n",
        "The actual matched text from the document.\n",
        "This code is useful for analyzing the results of pattern matching and understanding which patterns were matched and where they occurred in the text. Would you like to dive deeper into pattern matching or how to define more complex patterns?"
      ],
      "metadata": {
        "id": "bNUVisPCHYid"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8GnJPGvxwZJ"
      },
      "source": [
        "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRWP8S1YxwZJ"
      },
      "source": [
        "### Setting pattern options and quantifiers\n",
        "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3pJpQRIxwZJ"
      },
      "outputs": [],
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NLhu5ZwxwZK",
        "outputId": "839bff5a-f49b-49af-dcf5-1bd068494017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9bIsxFxwZK"
      },
      "source": [
        "This found both two-word patterns, with and without the hyphen!\n",
        "\n",
        "The following quantifiers can be passed to the `'OP'` key:\n",
        "<table><tr><th>OP</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcnXZXZuxwZK"
      },
      "source": [
        "### Be careful with lemmas!\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h3_EpRfxwZK"
      },
      "outputs": [],
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIK8vyvcxwZK"
      },
      "outputs": [],
      "source": [
        "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N80Q9p_GxwZK",
        "outputId": "ac285a37-d924-44ad-ba7c-1470b1a21e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 0, 3)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqk8Mv3xwZL"
      },
      "source": [
        "<font color=green>The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7X-zuUZxwZL"
      },
      "outputs": [],
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3, pattern4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAUuRQDNxwZL",
        "outputId": "c8496c0d-b1a0-48fb-a783-7461469a5cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ],
      "source": [
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mna1cJQexwZL"
      },
      "source": [
        "## Other token attributes\n",
        "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnWcWzzoxwZL"
      },
      "source": [
        "### Token wildcard\n",
        "You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",
        ">`[{'ORTH': '#'}, {}]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG__3Lj8xwZL"
      },
      "source": [
        "___\n",
        "## PhraseMatcher\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-dzrmYBxwZL"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHHZALcOxwZM"
      },
      "outputs": [],
      "source": [
        "# Import the PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJyacBMxwZM"
      },
      "source": [
        "For this exercise we're going to import a Wikipedia article on *Reaganomics*<br>\n",
        "Source: https://en.wikipedia.org/wiki/Reaganomics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRqLr3sTxwZM"
      },
      "outputs": [],
      "source": [
        "with open('../TextFiles/reaganomics.txt', encoding='utf8') as f:\n",
        "    doc3 = nlp(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqyyOszfxwZM"
      },
      "outputs": [],
      "source": [
        "# First, create a list of match phrases:\n",
        "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
        "\n",
        "# Next, convert each phrase to a Doc object:\n",
        "phrase_patterns = [nlp(text) for text in phrase_list]\n",
        "\n",
        "# Pass each Doc object into matcher (note the use of the asterisk!):\n",
        "matcher.add('VoodooEconomics', None, *phrase_patterns)\n",
        "\n",
        "# Build a list of matches:\n",
        "matches = matcher(doc3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtkGrxaSxwZM",
        "outputId": "664d5e2e-d28a-42e8-8a64-bb6c4a078f79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(3473369816841043438, 41, 45),\n",
              " (3473369816841043438, 49, 53),\n",
              " (3473369816841043438, 54, 56),\n",
              " (3473369816841043438, 61, 65),\n",
              " (3473369816841043438, 673, 677),\n",
              " (3473369816841043438, 2985, 2989)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (match_id, start, end)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6q0ByqMxwZM"
      },
      "source": [
        "<font color=green>The first four matches are where these terms are used in the definition of Reaganomics:</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e437qbLmxwZN",
        "outputId": "e9968bd2-8e8b-45be-82b8-f3aac45573d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "REAGANOMICS\n",
              "https://en.wikipedia.org/wiki/Reaganomics\n",
              "\n",
              "Reaganomics (a portmanteau of [Ronald] Reagan and economics attributed to Paul Harvey)[1] refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc3[:70]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJbllNCxwZN"
      },
      "source": [
        "## Viewing Matches\n",
        "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYMwWtR5xwZN",
        "outputId": "36b7c5f1-c954-4c0d-9cbf-462d9e34ee0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip8fdtCBxwZO",
        "outputId": "713d4b2b-08d4-40f4-cb17-c93826176413"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "against institutions.[66] His policies became widely known as \"trickle-down economics\", due to the significant"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc3[2975:2995]  # The sixth match starts at doc3[2985]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5EpOQjxwZO"
      },
      "source": [
        "Another way is to first apply the `sentencizer` to the Doc, then iterate through the sentences to the match point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW_wZJ0ExwZO",
        "outputId": "de031a16-f350-4ae6-a5d1-27f353ab80e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 35\n"
          ]
        }
      ],
      "source": [
        "# Build a list of sentences\n",
        "sents = [sent for sent in doc3.sents]\n",
        "\n",
        "# In the next section we'll see that sentences contain start and end token values:\n",
        "print(sents[0].start, sents[0].end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i52hCfQFxwZO",
        "outputId": "f07e7abf-6ea4-4d41-8fcf-1154a2441c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics.\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the sentence list until the sentence end value exceeds a match start value:\n",
        "for sent in sents:\n",
        "    if matches[4][1] < sent.end:  # this is the fifth match, that starts at doc3[673]\n",
        "        print(sent)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_3P5rezxwZP"
      },
      "source": [
        "For additional information visit https://spacy.io/usage/linguistic-features#section-rule-based-matching\n",
        "## Next Up: NLP Basics Assessment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
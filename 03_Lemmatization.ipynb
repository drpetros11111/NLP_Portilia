{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/NLP_Portilia/blob/NLP_Spacy_Basics_1/03_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NXFxdWO75Pvx"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXHoeY-d5Pvz"
      },
      "source": [
        "# Lemmatization\n",
        "In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INelM3l95Pv0"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS04C0gR5Pv1",
        "outputId": "288a84f5-7c36-42d1-8263-793f7a90928b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t PRON \t 4690420944186131903 \t I\n",
            "am \t AUX \t 10382539506755952630 \t be\n",
            "a \t DET \t 11901859001352538922 \t a\n",
            "runner \t NOUN \t 12640964157389618806 \t runner\n",
            "running \t VERB \t 12767647472892411841 \t run\n",
            "in \t ADP \t 3002984154512732771 \t in\n",
            "a \t DET \t 11901859001352538922 \t a\n",
            "race \t NOUN \t 8048469955494714898 \t race\n",
            "because \t SCONJ \t 16950148841647037698 \t because\n",
            "I \t PRON \t 4690420944186131903 \t I\n",
            "love \t VERB \t 3702023516439754181 \t love\n",
            "to \t PART \t 3791531372978436496 \t to\n",
            "run \t VERB \t 12767647472892411841 \t run\n",
            "since \t SCONJ \t 10066841407251338481 \t since\n",
            "I \t PRON \t 4690420944186131903 \t I\n",
            "ran \t VERB \t 12767647472892411841 \t run\n",
            "today \t NOUN \t 11042482332948150395 \t today\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
        "\n",
        "for token in doc1:\n",
        "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCONJ stands for Subordinating Conjunction.\n",
        "\n",
        "A subordinating conjunction connects a subordinate (dependent) clause to a main (independent) clause.\n",
        "\n",
        "Here are some examples of subordinating conjunctions:\n",
        "\n",
        "    after\n",
        "    although\n",
        "    as\n",
        "    because\n",
        "    before\n",
        "    even if\n",
        "    even though\n",
        "    if\n",
        "    once\n",
        "    since\n",
        "    so that\n",
        "    than\n",
        "    that\n",
        "    though\n",
        "    unless\n",
        "    until\n",
        "    when\n",
        "    whenever\n",
        "    where\n",
        "    whereas\n",
        "    wherever\n",
        "    while\n",
        "For example, in the sentence \"I will eat if I am hungry\", \"if\" is a subordinating conjunction.\n",
        "\n",
        "Do you have any other questions about spaCy or NLP?"
      ],
      "metadata": {
        "id": "FLrxHcHs6cBs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhCk_VLk5Pv2"
      },
      "source": [
        "<font color=green>In the above sentence, `running`, `run` and `ran` all point to the same lemma `run` (...11841) to avoid duplication.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnqqKlY5Pv2"
      },
      "source": [
        "### Function to display lemmas\n",
        "Since the display above is staggared and hard to read, let's write a function that displays the information we want more neatly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6WQtBgf5Pv2"
      },
      "outputs": [],
      "source": [
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neatly Displays Lemmas\n",
        "This Python function show_lemmas(text) neatly displays the lemmas of each token in a given text using spaCy.\n",
        "\n",
        "---------------------\n",
        "Here's a breakdown:\n",
        "\n",
        "##def show_lemmas(text):\n",
        "\n",
        "This line defines a function named show_lemmas that takes a spaCy Doc object (text) as input.\n",
        "\n",
        "##for token in text:\n",
        "\n",
        "This line starts a loop that iterates through each token in the input Doc object.\n",
        "\n",
        "   print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
        "   \n",
        "This line prints information about each token using an f-string for formatting.\n",
        "\n",
        "##token.text:{12}:\n",
        "\n",
        "The token's original text, formatted to take up 12 spaces (left-aligned).\n",
        "\n",
        "##token.pos_:{6}:\n",
        "\n",
        "The token's part-of-speech tag (e.g., 'NOUN', 'VERB'), formatted to take up 6 spaces.\n",
        "\n",
        "##token.lemma:<{22}:\n",
        "\n",
        "The token's lemma (base form), formatted to take up 22 spaces (left-aligned).\n",
        "\n",
        "##token.lemma_:\n",
        "\n",
        "The lemma's hash value, which is an integer representation.\n",
        "\n",
        "This function helps to visualize the key attributes of each token, making it easier to understand the lemmatization process.\n",
        "\n",
        "For example, if you call show_lemmas with the following code:\n",
        "\n",
        "\n",
        "    doc = nlp(u\"I saw eighteen mice today!\")\n",
        "    show_lemmas(doc)\n",
        "\n",
        "-----------------------\n",
        "#This would be the output:\n",
        "\n",
        "\n",
        "    I            PRON   I                 5687\n",
        "    saw          VERB   see               11925\n",
        "    eighteen      NUM    eighteen          12435\n",
        "    mice         NOUN   mouse             5705\n",
        "    today        NOUN   today             12066\n",
        "    !            PUNCT  !                 975\n",
        "\n",
        "-------------\n",
        "#Summary\n",
        "This function is particularly helpful for demonstrating how spaCy lemmatizes words, providing a clear comparison between the original word form, its part of speech, and its base form (lemma)."
      ],
      "metadata": {
        "id": "7Yzl81Gm8BrN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGoAokGO5Pv2"
      },
      "source": [
        "Here we're using an **f-string** to format the printed text by setting minimum field widths and adding a left-align to the lemma hash value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE2k5Ass5Pv3",
        "outputId": "b00a2317-c9b2-4a8e-eee4-e72f78594b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "saw          VERB   11925638236994514241   see\n",
            "eighteen     NUM    9609336664675087640    eighteen\n",
            "mice         NOUN   1384165645700560590    mouse\n",
            "today        NOUN   11042482332948150395   today\n",
            "!            PUNCT  17494803046312582752   !\n"
          ]
        }
      ],
      "source": [
        "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
        "\n",
        "show_lemmas(doc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKWCCnko5Pv3"
      },
      "source": [
        "<font color=green>Notice that the lemma of `saw` is `see`, `mice` is the plural form of `mouse`, and yet `eighteen` is its own number, *not* an expanded form of `eight`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elek2yjf5Pv3",
        "outputId": "09f8f153-7cea-44f9-dc07-ae9d8c8a4771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "am           AUX    10382539506755952630   be\n",
            "meeting      VERB   6880656908171229526    meet\n",
            "him          PRON   1655312771067108281    he\n",
            "tomorrow     NOUN   3573583789758258062    tomorrow\n",
            "at           ADP    11667289587015813222   at\n",
            "the          DET    7425985699627899538    the\n",
            "meeting      NOUN   14798207169164081740   meeting\n",
            ".            PUNCT  12646065887601541794   .\n"
          ]
        }
      ],
      "source": [
        "doc3 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n",
        "\n",
        "show_lemmas(doc3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB64u8R-5Pv3"
      },
      "source": [
        "<font color=green>Here the lemma of `meeting` is determined by its Part of Speech tag.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIF3bO-r5Pv4",
        "outputId": "27629bcd-78dd-4a8c-cb3f-84ec52854d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That         PRON   4380130941430378203    that\n",
            "'s           AUX    10382539506755952630   be\n",
            "an           DET    15099054000809333061   an\n",
            "enormous     ADJ    17917224542039855524   enormous\n",
            "automobile   NOUN   7211811266693931283    automobile\n"
          ]
        }
      ],
      "source": [
        "doc4 = nlp(u\"That's an enormous automobile\")\n",
        "\n",
        "show_lemmas(doc4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM90UgtF5Pv4"
      },
      "source": [
        "<font color=green>Note that lemmatization does *not* reduce words to their most basic synonym - that is, `enormous` doesn't become `big` and `automobile` doesn't become `car`.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNhJbGO25Pv4"
      },
      "source": [
        "We should point out that although lemmatization looks at surrounding text to determine a given word's part of speech, it does not categorize phrases. In an upcoming lecture we'll investigate *word vectors and similarity*.\n",
        "\n",
        "## Next up: Stop Words"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}